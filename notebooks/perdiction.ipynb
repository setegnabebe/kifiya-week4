{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                Prediction of store sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, filename='model_training.log', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Cleaned Training Data\n",
    "train_data = pd.read_csv('../data/cleaned_train.csv', low_memory=False)  \n",
    "print(\"Data Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature Engineering\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "train_data['weekday'] = train_data['Date'].dt.weekday\n",
    "train_data['weekend'] = (train_data['weekday'] >= 5).astype(int)\n",
    "\n",
    "train_data['StateHoliday'] = train_data['StateHoliday'].apply(lambda x: 1 if x != '0' else 0)\n",
    "\n",
    "# Example holiday date (adjust as needed)\n",
    "train_data['holiday_date'] = pd.to_datetime('2023-12-25')  \n",
    "train_data['days_to_holiday'] = (train_data['holiday_date'] - train_data['Date']).dt.days\n",
    "train_data['days_after_holiday'] = (train_data['Date'] - train_data['holiday_date']).dt.days\n",
    "\n",
    "train_data['beginning_of_month'] = (train_data['Date'].dt.day <= 10).astype(int)\n",
    "train_data['mid_month'] = ((train_data['Date'].dt.day > 10) & (train_data['Date'].dt.day <= 20)).astype(int)\n",
    "train_data['end_of_month'] = (train_data['Date'].dt.day > 20).astype(int)\n",
    "\n",
    "print(\"Feature Engineering Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Promo', 'DayOfWeek', 'days_to_holiday', \n",
    "            'days_after_holiday', 'beginning_of_month', 'mid_month', \n",
    "            'end_of_month', 'Customers', 'StateHoliday']\n",
    "X = train_data[features]\n",
    "y = train_data['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "logging.info(\"Split data into training and validation sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "pipeline.fit(X_train, y_train)\n",
    "logging.info(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 24,
>>>>>>> 4037f55 (api for the model)
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = pipeline.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "logging.info(\"Validation MSE: {}\".format(mse))\n",
    "print(f\"Validation MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 25,
>>>>>>> 4037f55 (api for the model)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Model saved as model.joblib\n"
=======
      "Model saved as model.pkl\n"
>>>>>>> 4037f55 (api for the model)
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_filename = 'model.joblib'\n",
    "joblib.dump(pipeline, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imortance of features\n",
    "importances = pipeline.named_steps['model'].feature_importances_\n",
    "features_importance = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "features_importance.sort_values(by='importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=features_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Building a Deep Learning Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df=pd.read_csv('../data/cleaned_train.csv', low_memory=False)\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(df[['Sales']])\n",
    "\n",
    "# Prepare the dataset for LSTM\n",
    "def create_dataset(data, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back), 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 7  # You can adjust this based on the time window\n",
    "X, y = create_dataset(scaled_data, look_back)\n",
    "\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM\n",
    "\n",
    "# Build the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(look_back, 1)),\n",
    "    tf.keras.layers.LSTM(50),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "# Predict future sales\n",
    "predicted_sales = model.predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
